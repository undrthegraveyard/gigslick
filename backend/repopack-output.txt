This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-09-25T01:44:51.035Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
.upm/
  store.json
.breakpoints
.gitignore
.replit
index.js
package.json
replit_zip_error_log.txt
replit.nix

================================================================
Repository Files
================================================================

================
File: .upm/store.json
================
{"version":2,"languages":{"nodejs-npm":{"specfileHash":"d9f7623d00c1412388d50ae3379eb91b","lockfileHash":"619fd4cd70c6ed72491b823036e4dca6","guessedImports":["express","cors","openai","multer"],"guessedImportsHash":"eb1073dd4903d869db733e00e38e17ee"}}}

================
File: .breakpoints
================
{
  "files": {
    "index.js": [
      {
        "id": "bc3a8ff0-c242-4e64-8eed-249a5edae4c2",
        "line": 1,
        "version": 524,
        "index": 0
      }
    ]
  }
}

================
File: .gitignore
================
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
lerna-debug.log*
.pnpm-debug.log*

# Diagnostic reports (https://nodejs.org/api/report.html)
report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover
lib-cov

# Coverage directory used by tools like istanbul
coverage
*.lcov

# nyc test coverage
.nyc_output

# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)
.grunt

# Bower dependency directory (https://bower.io/)
bower_components

# node-waf configuration
.lock-wscript

# Compiled binary addons (https://nodejs.org/api/addons.html)
build/Release

# Dependency directories
node_modules/
jspm_packages/

# Snowpack dependency directory (https://snowpack.dev/)
web_modules/

# TypeScript cache
*.tsbuildinfo

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Optional stylelint cache
.stylelintcache

# Microbundle cache
.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# dotenv environment variable files
.env
.env.development.local
.env.test.local
.env.production.local
.env.local

# parcel-bundler cache (https://parceljs.org/)
.cache
.parcel-cache

# Next.js build output
.next
out

# Nuxt.js build / generate output
.nuxt
dist

# Gatsby files
.cache/
# Comment in the public line in if your project uses Gatsby and not Next.js
# https://nextjs.org/blog/next-9-1#public-directory-support
# public

# vuepress build output
.vuepress/dist

# vuepress v2.x temp and cache directory
.temp
.cache

# Docusaurus cache and generated files
.docusaurus

# Serverless directories
.serverless/

# FuseBox cache
.fusebox/

# DynamoDB Local files
.dynamodb/

# TernJS port file
.tern-port

# Stores VSCode versions used for testing VSCode extensions
.vscode-test

# yarn v2
.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.pnp.*

================
File: .replit
================
entrypoint = "index.js"
modules = ["nodejs-20"]
hidden = [".config", "package-lock.json"]

[gitHubImport]
requiredFiles = [".replit", "replit.nix", "package.json", "package-lock.json"]

[nix]
channel = "stable-24_05"

[unitTest]
language = "nodejs"

[deployment]
run = ["node", "index.js"]
deploymentTarget = "cloudrun"
ignorePorts = false

[[ports]]
localPort = 5000
externalPort = 80

================
File: index.js
================
import express from 'express';
import cors from 'cors';
import busboy from 'busboy';
import path from 'path';
import fs from 'fs';
import { fileURLToPath } from 'url';
import { dirname } from 'path';
import OpenAI from 'openai';
import dotenv from 'dotenv';

dotenv.config();

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const app = express();
app.use(express.json());

app.use(cors({
  origin: '*',
  methods: ['GET', 'POST', 'PUT', 'DELETE'],
  allowedHeaders: ['Content-Type', 'Authorization']
}));

const uploadsDir = path.join(__dirname, 'uploads');
if (!fs.existsSync(uploadsDir)){
    fs.mkdirSync(uploadsDir);
    console.log(`Created uploads directory: ${uploadsDir}`);
}

app.post('/api/upload-resume', (req, res) => {
  console.log("Received request for file upload");
  console.log("Request headers:", req.headers);

  const bb = busboy({ headers: req.headers });
  let saveToPath = '';
  let fileReceived = false;

  bb.on('file', (name, file, info) => {
    fileReceived = true;
    const { filename, encoding, mimeType } = info;
    console.log(`Processing file: [${name}]: filename: ${filename}, encoding: ${encoding}, mimeType: ${mimeType}`);
    
    const saveTo = path.join(uploadsDir, filename);
    saveToPath = saveTo;
    console.log(`Attempting to save file to: ${saveTo}`);

    const writeStream = fs.createWriteStream(saveTo);
    file.pipe(writeStream);

    file.on('data', (data) => {
      console.log(`Received ${data.length} bytes of data.`);
    });

    file.on('end', () => {
      console.log('File upload finished');
      res.json({ 
        message: 'File uploaded successfully',
        path: saveTo
      });
    });

    writeStream.on('error', (error) => {
      console.error(`Error saving file: ${error}`);
      res.status(500).json({ error: 'Error saving file' });
    });
  });

  bb.on('error', (error) => {
    console.error("Busboy error:", error);
    res.status(500).json({ error: 'File upload failed' });
  });

  bb.on('finish', () => {
    console.log("Busboy finished processing the request");
    if (!fileReceived) {
      console.error('No file was processed');
      res.status(400).json({ error: 'No file was processed' });
    }
  });

  req.pipe(bb);
});


// Job description structuring route
app.post('/api/structure-job-description', async (req, res) => {
  console.log('Received request body:', req.body);  // Add this line for debugging
  
  if (!req.body || !req.body.description) {
    return res.status(400).json({ error: 'Invalid request body. Description is required.' });
  }
  
  const { description } = req.body;

  if (!description) {
    return res.status(400).json({ error: 'Job description is required' });
  }

  try {
    console.log('Received job description:', description);

    const prompt = `
      Convert the following unstructured job description into a professional, well-articulated
      resume section.
      Use the exact format shown in the example below:
      Example:
      Christies People (Labourer)
      NOV 2023 - JUN 2024, Sydney
      • Operated power tools safely and effectively to complete tasks in a timely manner.
      • Assisted tradespeople as required, ensuring smooth project flow.
      • Managed site opening and closing, ensuring all protocols were followed.
      • Controlled site traffic when necessary, maintaining safe access and flow.
      Rules:
      1. Use the exact format shown above.
      2. Start each bullet point with a strong action verb in past tense.
      3. Focus on specific achievements, impacts, and quantifiable results where possible.
      4. Use professional language suitable for a construction industry resume.
      5. Limit to 4-5 bullet points maximum.
      6. Ensure all information is accurate based on the input provided.
      Unstructured Input: ${description}
    `;

    console.log('Sending prompt to OpenAI:', prompt);

    const response = await openai.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [{ role: "user", content: prompt }],
      temperature: 0.5,
      max_tokens: 300,
    });

    console.log('Received response from OpenAI:', response.choices[0].message.content);

    let structuredText = response.choices[0].message.content.trim();
    structuredText = structuredText.replace(/•/g, '- ');
    structuredText = structuredText.replace(/(\d{4}) - (\d{4})/g, '$1-$2');
    structuredText = structuredText.replace(/- (.+?)(?=\n|$)/g, (match, p1) => {
      return `- ${p1.charAt(0).toUpperCase() + p1.slice(1)}${p1.endsWith('.') ? '' : '.'}`;
    });

    console.log('Sending structured text back to client:', structuredText);

    res.json({ structuredText });
  } catch (error) {
    console.error('Detailed error:', error);
    if (error.response) {
      console.error('OpenAI API response:', error.response.data);
    }
    res.status(500).json({ 
      error: 'Failed to process job description', 
      details: error.message,
      openaiError: error.response ? error.response.data : null
    });
  }
});

//Server is running on port 5001
const PORT = process.env.PORT || 5001;
app.listen(PORT, () => {
  console.log(`Server is running on port ${PORT}`);
})

app.use((err, req, res, next) => {
  console.error('Unhandled error:', err);
  res.status(500).json({ 
    error: 'Internal server error', 
    details: err.message 
  });
});

================
File: package.json
================
{
  "name": "nodejs",
  "version": "1.0.0",
  "description": "",
  "type": "module",
  "main": "index.js",
  "scripts": {
    "start": "node index.js",
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "author": "",
  "license": "ISC",
  "dependencies": {
    "@types/node": "^18.0.6",
    "axios": "^1.7.5",
    "busboy": "^1.6.0",
    "cors": "^2.8.5",
    "dotenv": "^16.4.5",
    "express": "^4.19.2",
    "multer": "^1.4.5-lts.1",
    "openai": "^4.56.0"
  }
}

================
File: replit_zip_error_log.txt
================
{"error":".zip archives do not support non-regular files","level":"error","msg":"unable to write file .cache/replit/modules/nodejs-20","time":"2024-08-28T02:56:03Z"}
{"error":".zip archives do not support non-regular files","level":"error","msg":"unable to write file .cache/replit/modules/replit","time":"2024-08-28T02:56:03Z"}
{"error":".zip archives do not support non-regular files","level":"error","msg":"unable to write file node_modules/.bin/mime","time":"2024-08-28T02:56:04Z"}
{"error":".zip archives do not support non-regular files","level":"error","msg":"unable to write file node_modules/.bin/mkdirp","time":"2024-08-28T02:56:04Z"}
{"error":".zip archives do not support non-regular files","level":"error","msg":"unable to write file node_modules/.bin/openai","time":"2024-08-28T02:56:04Z"}

================
File: replit.nix
================
{pkgs}: {
  deps = [ ];
}
